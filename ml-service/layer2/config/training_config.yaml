# Layer 2 Meta-Classifier Training Configuration
# LoRA-adapted Llama 3 8B for risk assessment

model:
  name: "meta-llama/Meta-Llama-3-8B"
  test_name: "sshleifer/tiny-gpt2"
  max_length: 512

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
  test_target_modules:
    - "c_attn"

quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true
  bnb_4bit_compute_dtype: "float16"

training:
  batch_size: 4
  gradient_accumulation_steps: 8
  num_epochs: 3
  learning_rate: 2.0e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  weight_decay: 0.001
  max_grad_norm: 0.3
  fp16: true
  logging_steps: 10
  save_steps: 100
  eval_steps: 100
  # Test mode overrides
  test_batch_size: 2
  test_max_steps: 2
  test_num_epochs: 1

data:
  train_file: "data/train.ndjson"
  val_split: 0.1
  max_length: 512
  mock_num_examples: 100

wandb:
  enabled: false
  project: "layer2-meta-classifier"
  entity: null
  run_name: null
